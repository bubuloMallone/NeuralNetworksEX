{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdpOzCyYmr5R2Vm+lDIX8k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bubuloMallone/NeuralNetworksLM/blob/main/backprop_tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the Backpropagation Algorithm\n",
        "\n",
        "The aim of this notebook is to implement from scratch the back propagation alogorithm, this time at the tensors level (like the PyTorch .backprop() method) to understand it works under the hood. This helps also understanding how to use it optimally in further training."
      ],
      "metadata": {
        "id": "xERYumr0YUVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "28GK1_DdYTTf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read all the words dataset\n",
        "\n",
        "!wget https://raw.githubusercontent.com/bubuloMallone/NeuralNetworksLM/refs/heads/main/datasets/names.txt\n",
        "\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "\n",
        "words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nk__1tjfbZk8",
        "outputId": "81fa0596-0469-4e10-d898-45a1c601f840"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-15 09:33:09--  https://raw.githubusercontent.com/bubuloMallone/NeuralNetworksLM/refs/heads/main/datasets/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-07-15 09:33:09 (6.19 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Daklhel6bk3g",
        "outputId": "32956c2c-37d1-4250-a12a-8840a345434e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "\n",
        "# define the context length: how many char we consider to predict the next one\n",
        "block_size = 3\n",
        "\n",
        "def build_dataset(words):\n",
        "\n",
        "  X, Y = [], []\n",
        "  for word in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in word + '.':\n",
        "      idx = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(idx)\n",
        "      # print(''.join(itos[i] for i in context), '--->', itos[idx])\n",
        "      context = context[1:] + [idx]\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print('Data:', X.shape, X.dtype)\n",
        "  print('Labels:', Y.shape, Y.dtype)\n",
        "  num_samples = X.shape[0]\n",
        "\n",
        "  return X, Y, num_samples\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr, num_samples_tr = build_dataset(words[:n1])\n",
        "Xval, Yval, num_samples_val = build_dataset(words[n1:n2])\n",
        "Xte, Yte, num_samples_te = build_dataset(words[n2:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgVZSOq_cIn5",
        "outputId": "37c20b7b-b4b8-457d-e324-7c07bd3893fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: torch.Size([182625, 3]) torch.int64\n",
            "Labels: torch.Size([182625]) torch.int64\n",
            "Data: torch.Size([22655, 3]) torch.int64\n",
            "Labels: torch.Size([22655]) torch.int64\n",
            "Data: torch.Size([22866, 3]) torch.int64\n",
            "Labels: torch.Size([22866]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let us define a utility function to compare the manual gradients computed to PyTorch gradients."
      ],
      "metadata": {
        "id": "t2PReI9KcpFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to compare gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n"
      ],
      "metadata": {
        "id": "eLsIHs6Dcitu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us initialize the neular network (MLP) as usual"
      ],
      "metadata": {
        "id": "H1idjqf1eD9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP architecture\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "emb_dim = 10  # the dimensionality of the character embedding vectors\n",
        "hidden_dim = 200  # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "# embedding\n",
        "C = torch.randn((vocab_size, emb_dim))\n",
        "# Layer 1\n",
        "W1 = torch.randn((block_size * emb_dim, hidden_dim), generator=g) * (5/3)/((block_size * emb_dim)**0.5) # Kaiming init for tanh! * 0.2  # to avoid dead neurons\n",
        "b1 = torch.randn(hidden_dim, generator=g) * 0.1  # keep b1 just for fun (check the grads) even though it is actually useless\n",
        "# Layer 2\n",
        "W2 = torch.randn((hidden_dim, vocab_size), generator=g) * 0.1  # to keep low initial logits\n",
        "b2 = torch.randn(vocab_size, generator=g) * 0.1   # to keep low initial logits\n",
        "# BatchNorm parameters\n",
        "bn_gain = torch.randn((1, hidden_dim)) * 0.1 + 1.0\n",
        "bn_bias = torch.randn((1, hidden_dim)) * 0.1\n",
        "\n",
        "# Note: many of these parameters are initialized in non-standard ways because the correct\n",
        "# initializations sometimes might mask some incorrect implementations of the backward pass\n",
        "\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
        "\n",
        "# require gradients\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "tot_parameters = sum(p.nelement() for p in parameters)\n",
        "print(f'Total number of parameters: {tot_parameters}')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFMQtzovd97-",
        "outputId": "e339508d-7d30-4e8b-91d6-db88c200a119"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 12297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "# minibatch construct\n",
        "idxs = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)  # (num_samples,) --> (batch_size,)\n",
        "Xb, Yb = Xtr[idxs], Ytr[idxs]  # (batch_size, block_size) and (batch_size,)"
      ],
      "metadata": {
        "id": "Ex8qQLV7tP_q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the forward pass in small steps sothat is possible to backward at every step."
      ],
      "metadata": {
        "id": "l3_aiKiptcZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "emb = C[Xb]   # (batch_size, block_size, emb_dim)\n",
        "emb_cat = emb.view(emb.shape[0], -1)  # (batch_size, block_size * emb_dim)\n",
        "# linear layer\n",
        "h_preact = emb_cat @ W1 + b1   # (batch_size, hidden_dim)\n",
        "# batch norm layer\n",
        "bn_mean_i = h_preact.mean(0, keepdim=True)\n",
        "bn_std_i = h_preact.std(0, keepdim=True)\n",
        "h_preact = batch_norm_gain * ((h_preact - bn_mean_i) / bn_std_i) + batch_norm_bias   # normalize the batch to unit gaussian and then offset/scale it according to learned bn_bias/gain (might add +eps small to std)\n",
        "with torch.no_grad():\n",
        "  bn_mean_running = 0.999 * bn_mean_running + 0.001 * bn_mean_i  # estimate running mean/std for later inference\n",
        "  bn_std_running = 0.999 * bn_std_running + 0.001 * bn_std_i    # 0.001 is the 'momentum'\n",
        "# non-linear activation\n",
        "h = torch.tanh(h_preact)   # (batch_size, hidden_dim)\n",
        "# output layer\n",
        "logits = h @ W2 + b2    # (batch_size, alphabet_size)\n",
        "loss = F.cross_entropy(logits, Yb)\n",
        "\n",
        "  # Backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update parameters\n",
        "  learning_rate = 0.1 if i < 100000 else 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -learning_rate * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0 : # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  losses.append(loss.log10().item())\n"
      ],
      "metadata": {
        "id": "cKY_Kn3kg_ms"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}